{"cells":[{"source":"![Credit card being held in hand](credit_card.jpg)\n\nCommercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n### The Data\n\nThe data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"c3d71707-6966-4295-b716-74fc18d9756d","nodeType":"const"}},"chartState":{"chartModel":{"modelType":"range","chartId":"id-1cetda5fv9l","chartType":"groupedColumn","chartThemeName":"datalabTheme","chartOptions":{"common":{"animation":{"enabled":true}}},"chartPalette":{"fills":["#6568A0","#43D7A4","#4095DB","#FACC5F","#CAE279","#F08083","#5BCDF2","#F099DC","#965858","#7DB64F","#A98954"],"strokes":["#6568A0","#43D7A4","#4095DB","#FACC5F","#CAE279","#F08083","#5BCDF2","#F099DC","#965858","#7DB64F","#A98954"],"up":{"fill":"#459d55","stroke":"#1e652e"},"down":{"fill":"#ef5452","stroke":"#a82529"},"neutral":{"fill":"#b5b5b5","stroke":"#575757"},"altUp":{"fill":"#5090dc","stroke":"#2b5c95"},"altDown":{"fill":"#ffa03a","stroke":"#cc6f10"},"altNeutral":{"fill":"#b5b5b5","stroke":"#575757"}},"cellRange":{"rowStartIndex":null,"rowStartPinned":null,"rowEndIndex":null,"rowEndPinned":null,"columns":[]},"switchCategorySeries":false,"suppressChartRanges":false,"unlinkChart":false,"version":"32.2.2"},"rangeChartModel":{"rangeColumns":[],"switchCategorySeries":false}}}},"lastExecutedByKernel":null,"visualizeDataframe":false,"version":"ag-charts-v1"},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"0","type":"string"},{"name":"1","type":"string"},{"name":"2","type":"number"},{"name":"3","type":"string"},{"name":"4","type":"string"},{"name":"5","type":"string"},{"name":"6","type":"string"},{"name":"7","type":"number"},{"name":"8","type":"string"},{"name":"9","type":"string"},{"name":"10","type":"integer"},{"name":"11","type":"string"},{"name":"12","type":"integer"},{"name":"13","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"0":["b","a","a","b","b"],"1":["30.83","58.67","24.50","27.83","20.17"],"2":[0,4.46,0.5,1.54,5.625],"3":["u","u","u","u","u"],"4":["g","g","g","g","g"],"5":["w","q","q","w","w"],"6":["v","h","h","v","v"],"7":[1.25,3.04,1.5,3.75,1.71],"8":["t","t","t","t","t"],"9":["t","t","f","t","f"],"10":[1,6,0,5,0],"11":["g","g","g","g","s"],"12":[0,560,824,3,0],"13":["+","+","+","+","+"],"index":[0,1,2,3,4]}},"total_rows":5,"truncation_type":null},"text/plain":"  0      1      2  3  4  5  6     7  8  9   10 11   12 13\n0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  g    0  +\n1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  g  560  +\n2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  g  824  +\n3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  g    3  +\n4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  s    0  +","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b</td>\n      <td>30.83</td>\n      <td>0.000</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.25</td>\n      <td>t</td>\n      <td>t</td>\n      <td>1</td>\n      <td>g</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>58.67</td>\n      <td>4.460</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>3.04</td>\n      <td>t</td>\n      <td>t</td>\n      <td>6</td>\n      <td>g</td>\n      <td>560</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>24.50</td>\n      <td>0.500</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>1.50</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>g</td>\n      <td>824</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b</td>\n      <td>27.83</td>\n      <td>1.540</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>3.75</td>\n      <td>t</td>\n      <td>t</td>\n      <td>5</td>\n      <td>g</td>\n      <td>3</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b</td>\n      <td>20.17</td>\n      <td>5.625</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.71</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>s</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":2}]},{"source":"print(cc_apps.isna().sum().sort_values())","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1744885457282,"lastExecutedByKernel":"72319481-5e59-4ad2-a3f3-a47886dde470","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(cc_apps.isna().sum().sort_values())","outputsMetadata":{"0":{"height":332,"type":"stream"}}},"cell_type":"code","id":"47c012a1-9be4-4179-bec8-c9b2041c06a4","outputs":[{"output_type":"stream","name":"stdout","text":"0     0\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n11    0\n12    0\n13    0\ndtype: int64\n"}],"execution_count":3},{"source":"ccdf1 = cc_apps\n#replacing empty cells\nccdf2 = ccdf1.replace('?', np.nan)\nprint(ccdf2.isna().sum().sort_values())\nfor col in ccdf2.columns:\n    if ccdf2[col].dtype == 'object':\n        ccdf2[col] = ccdf2[col].fillna(ccdf2[col].mode()[0])\n    else:\n        ccdf2[col] = ccdf2[col].fillna(ccdf2[col].mean())\n\nccdf2_encoded = pd.get_dummies(ccdf2, drop_first=True)\n\nprint(ccdf2_encoded)\n# preparing data for modelling\n# Encode all object columns with LabelEncoder\ndf=ccdf2_encoded.copy()\n\n# Features (all columns except the last)\nX = df.iloc[:, :-1].values\n# Target (the last column)\ny = df.iloc[:, [-1]].values\n\nX_train, X_test,y_train, y_test=train_test_split(X,y,test_size=0.33,random_state=42)\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\nX_test_scaled = scaler.transform(X_test)\nlogreg=LogisticRegression()\nlogreg.fit(X_train_scaled, y_train)\ny_train_pred=logreg.predict(X_train_scaled)\nprint(confusion_matrix(y_train,y_train_pred))\n","metadata":{"executionCancelledAt":null,"executionTime":222,"lastExecutedAt":1744885457504,"lastExecutedByKernel":"72319481-5e59-4ad2-a3f3-a47886dde470","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"ccdf1 = cc_apps\n#replacing empty cells\nccdf2 = ccdf1.replace('?', np.nan)\nprint(ccdf2.isna().sum().sort_values())\nfor col in ccdf2.columns:\n    if ccdf2[col].dtype == 'object':\n        ccdf2[col] = ccdf2[col].fillna(ccdf2[col].mode()[0])\n    else:\n        ccdf2[col] = ccdf2[col].fillna(ccdf2[col].mean())\nccdf2_encoded = pd.get_dummies(ccdf2, drop_first=True)\n\nprint(ccdf2_encoded)\n# preparing data for modelling\n# Encode all object columns with LabelEncoder\ndf=ccdf2_encoded.copy()\n\n# Features (all columns except the last)\nX = df.iloc[:, :-1].values\n# Target (the last column)\ny = df.iloc[:, [-1]].values\n\nX_train, X_test,y_train, y_test=train_test_split(X,y,test_size=0.33,random_state=42)\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\nX_test_scaled = scaler.transform(X_test)\nlogreg=LogisticRegression()\nlogreg.fit(X_train_scaled, y_train)\ny_train_pred=logreg.predict(X_train_scaled)\nprint(confusion_matrix(y_train,y_train_pred))\n","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"361402e0-dfe9-4030-9fea-ab7607fe50fb","outputs":[{"output_type":"stream","name":"stdout","text":"2      0\n7      0\n8      0\n9      0\n10     0\n11     0\n12     0\n13     0\n3      6\n4      6\n5      9\n6      9\n0     12\n1     12\ndtype: int64\n          2     7  10   12  0_b  1_15.17  ...  6_z  8_t  9_t  11_p  11_s  13_-\n0     0.000  1.25   1    0    1        0  ...    0    1    1     0     0     0\n1     4.460  3.04   6  560    0        0  ...    0    1    1     0     0     0\n2     0.500  1.50   0  824    0        0  ...    0    1    0     0     0     0\n3     1.540  3.75   5    3    1        0  ...    0    1    1     0     0     0\n4     5.625  1.71   0    0    1        0  ...    0    1    0     0     1     0\n..      ...   ...  ..  ...  ...      ...  ...  ...  ...  ...   ...   ...   ...\n685  10.085  1.25   0    0    1        0  ...    0    0    0     0     0     1\n686   0.750  2.00   2  394    0        0  ...    0    0    1     0     0     1\n687  13.500  2.00   1    1    0        0  ...    0    0    1     0     0     1\n688   0.205  0.04   0  750    1        0  ...    0    0    0     0     0     1\n689   3.375  8.29   0    0    1        0  ...    0    0    0     0     0     1\n\n[690 rows x 383 columns]\n[[203   1]\n [  1 257]]\n"}],"execution_count":4},{"source":"print(cc_apps)","metadata":{"executionCancelledAt":null,"executionTime":66,"lastExecutedAt":1744885457570,"lastExecutedByKernel":"72319481-5e59-4ad2-a3f3-a47886dde470","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(cc_apps)","outputsMetadata":{"0":{"height":311,"type":"stream"}}},"cell_type":"code","id":"b874b4f2-9ad8-4964-b937-2a0f0e8d0b72","outputs":[{"output_type":"stream","name":"stdout","text":"    0      1       2  3  4   5   6     7  8  9   10 11   12 13\n0    b  30.83   0.000  u  g   w   v  1.25  t  t   1  g    0  +\n1    a  58.67   4.460  u  g   q   h  3.04  t  t   6  g  560  +\n2    a  24.50   0.500  u  g   q   h  1.50  t  f   0  g  824  +\n3    b  27.83   1.540  u  g   w   v  3.75  t  t   5  g    3  +\n4    b  20.17   5.625  u  g   w   v  1.71  t  f   0  s    0  +\n..  ..    ...     ... .. ..  ..  ..   ... .. ..  .. ..  ... ..\n685  b  21.08  10.085  y  p   e   h  1.25  f  f   0  g    0  -\n686  a  22.67   0.750  u  g   c   v  2.00  f  t   2  g  394  -\n687  a  25.25  13.500  y  p  ff  ff  2.00  f  t   1  g    1  -\n688  b  17.92   0.205  u  g  aa   v  0.04  f  f   0  g  750  -\n689  b  35.00   3.375  u  g   c   h  8.29  f  f   0  g    0  -\n\n[690 rows x 14 columns]\n"}],"execution_count":5},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None)\n\n# Replace the '?'s with NaN in dataset\ncc_apps_nans_replaced = cc_apps.replace(\"?\", np.NaN)\n\n# Create a copy of the NaN replacement DataFrame\ncc_apps_imputed = cc_apps_nans_replaced.copy()\n\n# Iterate over each column of cc_apps_nans_replaced and impute the most frequent value for object data types and the mean for numeric data types\nfor col in cc_apps_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(\n            cc_apps_imputed[col].value_counts().index[0]\n        )\n    else:\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(cc_apps_imputed[col].mean())\n\n# Dummify the categorical features\ncc_apps_encoded = pd.get_dummies(cc_apps_imputed, drop_first=True)\n\n# Extract the last column as your target variable\nX = cc_apps_encoded.iloc[:, :-1].values\ny = cc_apps_encoded.iloc[:, [-1]].values\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Instantiate StandardScaler and use it to rescale X_train and X_test\nscaler = StandardScaler()\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the training set\ny_train_pred = logreg.predict(rescaledX_train)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_train, y_train_pred))\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_train_score, best_train_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_train_score, best_train_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nbest_score =  best_model.score(rescaledX_test, y_test)\n\nprint(\"Accuracy of logistic regression classifier: \", best_score)","metadata":{"executionCancelledAt":null,"executionTime":6932,"lastExecutedAt":1744885464502,"lastExecutedByKernel":"72319481-5e59-4ad2-a3f3-a47886dde470","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None)\n\n# Replace the '?'s with NaN in dataset\ncc_apps_nans_replaced = cc_apps.replace(\"?\", np.NaN)\n\n# Create a copy of the NaN replacement DataFrame\ncc_apps_imputed = cc_apps_nans_replaced.copy()\n\n# Iterate over each column of cc_apps_nans_replaced and impute the most frequent value for object data types and the mean for numeric data types\nfor col in cc_apps_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(\n            cc_apps_imputed[col].value_counts().index[0]\n        )\n    else:\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(cc_apps_imputed[col].mean())\n\n# Dummify the categorical features\ncc_apps_encoded = pd.get_dummies(cc_apps_imputed, drop_first=True)\n\n# Extract the last column as your target variable\nX = cc_apps_encoded.iloc[:, :-1].values\ny = cc_apps_encoded.iloc[:, [-1]].values\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Instantiate StandardScaler and use it to rescale X_train and X_test\nscaler = StandardScaler()\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the training set\ny_train_pred = logreg.predict(rescaledX_train)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_train, y_train_pred))\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_train_score, best_train_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_train_score, best_train_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nbest_score =  best_model.score(rescaledX_test, y_test)\n\nprint(\"Accuracy of logistic regression classifier: \", best_score)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"cell_type":"code","id":"7125e91a-22e0-42f1-bce8-8dc6f1b0f3b9","outputs":[{"output_type":"stream","name":"stdout","text":"[[203   1]\n [  1 257]]\nBest: 0.818163 using {'max_iter': 100, 'tol': 0.01}\nAccuracy of logistic regression classifier:  0.793859649122807\n"}],"execution_count":6}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}